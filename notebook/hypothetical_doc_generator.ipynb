{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content in each cell\n",
    "pd.set_option('display.width', 1000)  # Set max width\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Define interrogative words to KEEP\n",
    "    interrogatives = {\"what\", \"why\", \"how\", \"who\", \"where\", \"when\", \"which\", \"whom\", \"whose\", \"no\", \"not\",\n",
    "                    \"very\" ,\"too\" ,\"too\" ,\"just\", \"if\", \"but\", \"however\", \"without\", \"like\"}\n",
    "    custom_stopwords = set(nlp.Defaults.stop_words)\n",
    "    custom_stopwords -= interrogatives\n",
    "\n",
    "    doc = nlp(text.lower().strip())  # Lowercase and remove whitespace\n",
    "    \n",
    "# Process tokens: lemmatize, filter stopwords/punct/numbers, keep interrogatives\n",
    "    tokens = [\n",
    "        token.lemma_ \n",
    "        for token in doc \n",
    "        if (\n",
    "            (not token.is_stop or token.text in interrogatives) and  # Keep interrogatives\n",
    "            not token.is_punct and token.is_alpha                                  # Remove punctuation\n",
    "            # (token.is_alpha or token.like_num)                       # Keep words/numbers\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapper = {\n",
    "    'BT1' : 'knowledge',\n",
    "    'BT2' : 'comprehension',\n",
    "    'BT3' : 'application',\n",
    "    'BT4' : 'analysis',\n",
    "    'BT5' : 'synthesis',\n",
    "    'BT6' : 'evaluation'\n",
    "}\n",
    "\n",
    "# Load dataset\n",
    "df = pd.DataFrame()\n",
    "for i in range(1,5):\n",
    "    q_df = pd.read_csv(os.getcwd().replace('notebook' , 'dataset') + '/dataset' + str(i) + '.csv')\n",
    "    df = pd.concat([df , q_df])\n",
    "\n",
    "# Apply preprocessing\n",
    "mask = df['label'].isin(label_mapper.keys())\n",
    "df['label'] = df['label'].mask(mask, df['label'].map(label_mapper))\n",
    "\n",
    "df['label'] = df['label'].str.lower()\n",
    "\n",
    "df['processed_question'] = df['question'].apply(preprocess_text)\n",
    "\n",
    "df['processed_question'] = [''.join(text) for text in df['processed_question']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothetical Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Configure for MPS acceleration (Apple Silicon)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "dtype = torch.float32  # MPS currently better with full precision\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOOGLE MODEL TEXT2TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files:   0%|          | 0/5 [11:39<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-xxl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator = pipeline(\n",
    "    task= \"text2text-generation\",\n",
    "    model= model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device = \"mps\"\n",
    "    )\n",
    "\n",
    "prompt = prompt = \"\"\"Generate 5 questions similar to 'why sky is blue?' satisfying blooms level 'understanding'. Follow this exact format:\n",
    "\n",
    "1. Why does the sky appear blue during daytime?\n",
    "2. How does light scattering affect sunset colors?\n",
    "...\n",
    "5.\n",
    "\n",
    " there should be 5 different question\"\"\"\n",
    "\n",
    "results = generator(\n",
    "    prompt,\n",
    "    max_length=500,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7)\n",
    "\n",
    "if results:\n",
    "    generated = results[0][\"generated_text\"].strip()\n",
    "    questions = []\n",
    "    for line in generated.split(\"\\n\"):\n",
    "        if line.strip() and any(c.isdigit() for c in line[:3]):\n",
    "            # Extract question text after number\n",
    "            q = line.split(\". \", 1)[-1].strip()\n",
    "            if q and q[-1] == \"?\":  # Simple validation\n",
    "                questions.append(q)\n",
    "    \n",
    "    print(f\"Generated ({len(questions)} questions):\")\n",
    "    for i, q in enumerate(questions[:10], 1):\n",
    "        print(f\"{i}. {q}\", end= \"\")\n",
    "else:\n",
    "    print(\"Generation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Why does the sky appear blue during daytime?', 'What causes the sunset colors?', 'What is the difference between daytime and sunset?', 'What is the difference between daytime and sunset?']\n"
     ]
    }
   ],
   "source": [
    "questions = re.findall(r\"\\d+[\\.\\)]\\s*(.*?\\?)\", generated)\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at openai-community/roberta-large-openai-detector and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated (0 questions):\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai-community/roberta-large-openai-detector\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator = pipeline(\n",
    "    task= \"text-generation\",\n",
    "    model= model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device = \"mps\"\n",
    "    )\n",
    "\n",
    "prompt = prompt = \"\"\"Generate 5 questions similar to 'why sky is blue?' satisfying blooms level 'understanding'. Follow this exact format:\n",
    "\n",
    "1. Why does the sky appear blue during daytime?\n",
    "2. How does light scattering affect sunset colors?\n",
    "...\n",
    "5.\n",
    "\n",
    " there should be 5 different question\"\"\"\n",
    "\n",
    "results = generator(\n",
    "    prompt,\n",
    "    max_length=500,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7)\n",
    "\n",
    "if results:\n",
    "    generated = results[0][\"generated_text\"].strip()\n",
    "    questions = []\n",
    "    for line in generated.split(\"\\n\"):\n",
    "        if line.strip() and any(c.isdigit() for c in line[:3]):\n",
    "            # Extract question text after number\n",
    "            q = line.split(\". \", 1)[-1].strip()\n",
    "            if q and q[-1] == \"?\":  # Simple validation\n",
    "                questions.append(q)\n",
    "    \n",
    "    print(f\"Generated ({len(questions)} questions):\")\n",
    "    for i, q in enumerate(questions[:10], 1):\n",
    "        print(f\"{i}. {q}\", end= \"\")\n",
    "else:\n",
    "    print(\"Generation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Why does the sky appear blue during daytime?', 'How does light scattering affect sunset colors?']\n"
     ]
    }
   ],
   "source": [
    "questions = re.findall(r\"\\d+[\\.\\)]\\s*(.*?\\?)\", generated)\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEEPSEEK MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"bigscience/bloom\"\n",
    "device = 'mps'\n",
    "prompt = prompt = \"\"\"Generate 5 short questions similar to 'why sky is blue?' satisfying blooms level 'understanding'. Follow this exact format:\n",
    "\n",
    "1. Why does the sky appear blue during daytime?\n",
    "2. How does light scattering affect sunset colors?\n",
    "...\n",
    "5.\n",
    "\n",
    "Questions:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "\n",
    "\n",
    "\n",
    "# Generation parameters\n",
    "results = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "if results:\n",
    "    generated = results[0][\"generated_text\"].strip()\n",
    "    questions = []\n",
    "    for line in generated.split(\"\\n\"):\n",
    "        if line.strip() and any(c.isdigit() for c in line[:3]):\n",
    "            # Extract question using multiple delimiters\n",
    "            parts = line.split(\". \", 1) if \". \" in line else line.split(\") \", 1)\n",
    "            if len(parts) > 1:\n",
    "                q = parts[1].strip()\n",
    "                if q and q[-1] == \"?\":\n",
    "                    questions.append(q[0].upper() + q[1:])  # Capitalize first letter\n",
    "    \n",
    "    print(f\"Generated ({len(questions)} questions):\")\n",
    "    for i, q in enumerate(questions[:10], 1):\n",
    "        print(f\"{i}. {q}\")\n",
    "else:\n",
    "    print(\"Generation failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_eve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
