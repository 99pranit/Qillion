{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/yt_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content in each cell\n",
    "pd.set_option('display.width', 1000)  # Set max width\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pranitdas/Library/Application Support/Open Interpreter\n"
     ]
    }
   ],
   "source": [
    "import appdirs \n",
    "print(appdirs.user_data_dir(\"Open Interpreter\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"uw-vta/bloominzer-0.1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"uw-vta/bloominzer-0.1\")\n",
    "bloominzer = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Synthesis', 'score': 0.9990537762641907}]\n"
     ]
    }
   ],
   "source": [
    "print(bloominzer(\"If I have 2 pair of apple, can i make apple pie with it?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "\n",
    "bart = pipeline(\"zero-shot-classification\",\n",
    "                      model=model , tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"If I have 2 pair of apple, can i make apple pie with it?\"\n",
    "candidate_labels = ['knowledge', 'comprehension', 'application', 'analysis','synthesis', 'evaluation']\n",
    "label = bart(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bart-lage-mnli-yahoo-answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"joeddav/bart-large-mnli-yahoo-answers\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"joeddav/bart-large-mnli-yahoo-answers\")\n",
    "\n",
    "ya_classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=model , tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"If I have 2 pair of apple, can i make apple pie with it?\"\n",
    "candidate_labels = ['knowledge', 'comprehension', 'application', 'analysis','synthesis', 'evaluation']\n",
    "hypothesis_template = \"This text is about blooms taxonomy and it is classified as {}.\"\n",
    "label = ya_classifier(sequence_to_classify, candidate_labels, hypothesis_template=hypothesis_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'knowledge'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"  # Requires GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W525 00:42:57.067232000 ProcessGroupGloo.cpp:757] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "\n",
    "dist.init_process_group(backend='gloo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded in 94.73 seconds\n"
     ]
    }
   ],
   "source": [
    "from models.llama3.generation import Llama\n",
    "import fire\n",
    "generator = Llama.build(\n",
    "        ckpt_dir=\"/Users/pranitdas/.llama/checkpoints/Llama3.1-8B-Instruct\",\n",
    "        tokenizer_path=\"/Users/pranitdas/.llama/checkpoints/Llama3.1-8B-Instruct/tokenizer.model\",\n",
    "        max_seq_len=8192,\n",
    "        max_batch_size=1, \n",
    "        device = 'mps',\n",
    "    world_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'role'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m dialog \u001b[38;5;241m=\u001b[39m [[{\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassify this question into Bloom\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Taxonomy: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is photosynthesis?\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m     }]]\n\u001b[0;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdialog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt_env/lib/python3.10/site-packages/models/llama3/generation.py:371\u001b[0m, in \u001b[0;36mLlama.chat_completion\u001b[0;34m(self, messages, temperature, top_p, max_gen_len, logprobs, tool_prompt_format, echo)\u001b[0m\n\u001b[1;32m    367\u001b[0m decoded_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    369\u001b[0m stop_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m--> 371\u001b[0m     model_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_dialog_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_prompt_format\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    372\u001b[0m     max_gen_len\u001b[38;5;241m=\u001b[39mmax_gen_len,\n\u001b[1;32m    373\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    374\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m    375\u001b[0m     logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[1;32m    376\u001b[0m     echo\u001b[38;5;241m=\u001b[39mecho,\n\u001b[1;32m    377\u001b[0m ):\n\u001b[1;32m    378\u001b[0m     tokens\u001b[38;5;241m.\u001b[39mappend(result\u001b[38;5;241m.\u001b[39mtoken)\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt_env/lib/python3.10/site-packages/models/llama3/chat_format.py:156\u001b[0m, in \u001b[0;36mChatFormat.encode_dialog_prompt\u001b[0;34m(self, messages, tool_prompt_format)\u001b[0m\n\u001b[1;32m    154\u001b[0m tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mspecial_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|begin_of_text|>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[0;32m--> 156\u001b[0m     toks, imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_prompt_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     tokens\u001b[38;5;241m.\u001b[39mextend(toks)\n\u001b[1;32m    158\u001b[0m     images\u001b[38;5;241m.\u001b[39mextend(imgs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yt_env/lib/python3.10/site-packages/models/llama3/chat_format.py:111\u001b[0m, in \u001b[0;36mChatFormat.encode_message\u001b[0;34m(self, message, tool_prompt_format)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_message\u001b[39m(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m, message: RawMessage, tool_prompt_format: ToolPromptFormat\n\u001b[1;32m    110\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mint\u001b[39m], List[PIL_Image\u001b[38;5;241m.\u001b[39mImage]]:\n\u001b[0;32m--> 111\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_header(\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrole\u001b[49m)\n\u001b[1;32m    112\u001b[0m     images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_process_content\u001b[39m(c):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'role'"
     ]
    }
   ],
   "source": [
    "dialog = [[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Classify this question into Bloom's Taxonomy: 'What is photosynthesis?'\"\n",
    "    }]]\n",
    "    \n",
    "results = generator.chat_completion(dialog, temperature=0.1)\n",
    "print(results[0]['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
