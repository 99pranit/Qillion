{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import string as st\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import fasttext\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content in each cell\n",
    "pd.set_option('display.width', 1000)  # Set max width\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Define interrogative words to KEEP\n",
    "    interrogatives = {\"what\", \"why\", \"how\", \"who\", \"where\", \"when\", \"which\", \"whom\", \"whose\"}\n",
    "    custom_stopwords = set(nlp.Defaults.stop_words)\n",
    "    custom_stopwords -= interrogatives\n",
    "\n",
    "    doc = nlp(text.lower().strip())  # Lowercase and remove whitespace\n",
    "    \n",
    "# Process tokens: lemmatize, filter stopwords/punct/numbers, keep interrogatives\n",
    "    tokens = [\n",
    "        token.lemma_ \n",
    "        for token in doc \n",
    "        if (\n",
    "            (not token.is_stop or token.text in interrogatives) and  # Keep interrogatives\n",
    "            not token.is_punct and                                   # Remove punctuation\n",
    "            (token.is_alpha or token.like_num)                       # Keep words/numbers\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_linguistic_features(text):\n",
    "    doc = nlp(text)\n",
    "    features = {\n",
    "        'num_verbs': len([token for token in doc if token.pos_ == 'VERB']),\n",
    "        'num_nouns': len([token for token in doc if token.pos_ == 'NOUN']),\n",
    "        'sentence_length': len(doc),\n",
    "        'blooms_verb_present': any(token.text in {'analyze', 'evaluate', 'create'} for token in doc)\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(text):\n",
    "    words = text.strip().split()\n",
    "    vectors = [model.get_word_vector(w) for w in words]\n",
    "    if not vectors:\n",
    "        return np.zeros(300)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_filter(corpus, min_freq=10):\n",
    "    \"\"\"Creates a word frequency filter based on ACTUAL WORDS\"\"\"\n",
    "    # Count WORD frequencies\n",
    "    word_counter = Counter()\n",
    "\n",
    "    for text in corpus:\n",
    "        word_counter.update(text) \n",
    "    \n",
    "    return {word for word, count in word_counter.items() if count >= min_freq}\n",
    "\n",
    "def filter_text(text, word_filter):\n",
    "    \"\"\"Filters text using precomputed word filter\"\"\"\n",
    "    return [word for word in text if word in word_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapper = {\n",
    "    'BT1' : 'knowledge',\n",
    "    'BT2' : 'comprehension',\n",
    "    'BT3' : 'application',\n",
    "    'BT4' : 'analysis',\n",
    "    'BT5' : 'synthesis',\n",
    "    'BT6' : 'evaluation'\n",
    "}\n",
    "\n",
    "# Load dataset\n",
    "df = pd.DataFrame()\n",
    "for i in range(1,5):\n",
    "    q_df = pd.read_csv(os.getcwd().replace('notebook' , 'dataset') + '/dataset' + str(i) + '.csv')\n",
    "    df = pd.concat([df , q_df])\n",
    "\n",
    "# Apply preprocessing\n",
    "mask = df['label'].isin(label_mapper.keys())\n",
    "df['label'] = df['label'].mask(mask, df['label'].map(label_mapper))\n",
    "df['label'] = df['label'].str.lower()\n",
    "\n",
    "df['processed_question'] = df['question'].apply(preprocess_text)\n",
    "\n",
    "df['processed_question'] = [''.join(text) for text in df['processed_question']]\n",
    "\n",
    "# Create frequency filter\n",
    "min_frequency = 100\n",
    "freq_filter = create_word_filter(df['processed_question'], min_frequency)\n",
    "\n",
    "# Apply filter to all texts\n",
    "filtered_corpus = [filter_text(text, freq_filter) for text in df['processed_question']]\n",
    "\n",
    "# Convert back to strings\n",
    "df['processed_question'] = [''.join(text) for text in filtered_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "BERT model is used to retain context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT (TensorFlow version)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode\n",
    "inputs = tokenizer(df['question'].tolist(), return_tensors='tf', padding=True, truncation=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract embeddings (CLS token for sentence representation)\n",
    "sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "print(\"BERT Sentence Embedding Shape:\", sentence_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1000)\n",
    "reduced_embeddings = pca.fit_transform(sentence_embedding.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF \n",
    "Execute either BERT or IF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10708, 7218)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(df['processed_question'])\n",
    "\n",
    "# Convert TF-IDF sparse matrix to DataFrame with appropriate column names\n",
    "tfidf_df = pd.DataFrame(tfidf.toarray(), columns= vectorizer.get_feature_names_out(), index=df.index)\n",
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute if linguistic features like count for verb,noun,etc needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding linguistic features\n",
    "ling_features = df['processed_question'].apply(extract_linguistic_features).apply(pd.Series)\n",
    "\n",
    "# Concatenate DataFrames\n",
    "token_df = pd.concat([ling_features, tfidf_df], axis=1)\n",
    "\n",
    "# Drop unnecessary columns (if needed)\n",
    "token_df = token_df.drop(columns=['blooms_verb_present'], axis=1)\n",
    "token_df.columns = token_df.columns.map(str)\n",
    "\n",
    "# token_df = pca.fit_transform(token_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x , y = tfidf_df , df['label']\n",
    "x_resampled , y_resampled = SMOTETomek().fit_resample(x , y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Machine Leanring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=34 , stratify= y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.816777</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.813710</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.805532</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.788515</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.765544</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682562</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.462598</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'n_estimators': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.386170</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'n_estimators': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.312868</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'n_estimators': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.251233</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'n_estimators': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score                                                         params\n",
       "5         0.816777  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 100}\n",
       "4         0.813710   {'criterion': 'gini', 'max_depth': None, 'n_estimators': 50}\n",
       "3         0.805532   {'criterion': 'gini', 'max_depth': None, 'n_estimators': 20}\n",
       "2         0.788515   {'criterion': 'gini', 'max_depth': None, 'n_estimators': 10}\n",
       "1         0.765544    {'criterion': 'gini', 'max_depth': None, 'n_estimators': 5}\n",
       "0         0.682562    {'criterion': 'gini', 'max_depth': None, 'n_estimators': 2}\n",
       "9         0.462598      {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 20}\n",
       "8         0.386170      {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 10}\n",
       "7         0.312868       {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 5}\n",
       "6         0.251233       {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 2}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators' : [2 , 5 , 10 , 20 , 50 , 100],\n",
    "    'criterion' : ['gini' , 'entropy'],\n",
    "    'max_depth' : [None , 5 , 10 , 20 , 50 , 100 , 200 , 350 , 500]\n",
    "}\n",
    "param_search = GridSearchCV(estimator= RandomForestClassifier(), param_grid= params, cv=5)\n",
    "\n",
    "param_search.fit(x_resampled , y_resampled)\n",
    "\n",
    "cv_results = pd.DataFrame(param_search.cv_results_)\n",
    "cv_results = cv_results[[\"mean_test_score\", \"params\"]].head(10).sort_values(by=\"mean_test_score\", ascending=False)\n",
    "\n",
    "cv_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     analysis       0.90      0.88      0.89       564\n",
      "  application       0.90      0.89      0.90       564\n",
      "comprehension       0.84      0.83      0.83       554\n",
      "   evaluation       0.81      0.78      0.79       559\n",
      "    knowledge       0.74      0.80      0.77       547\n",
      "    synthesis       0.79      0.80      0.79       558\n",
      "\n",
      "     accuracy                           0.83      3346\n",
      "    macro avg       0.83      0.83      0.83      3346\n",
      " weighted avg       0.83      0.83      0.83      3346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators= 100 , max_depth= None , criterion= 'gini')\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "print(classification_report(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['application']\n"
     ]
    }
   ],
   "source": [
    "question = 'How many total disk access is needed to search a record using two level indexing?'\n",
    "processed_q = preprocess_text(question)\n",
    "tfidf_q = vectorizer.transform([processed_q])\n",
    "tfidf_q = pd.DataFrame(tfidf_q.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# Adding linguistic features\n",
    "q_ling_features = pd.DataFrame([extract_linguistic_features(question)])\n",
    "\n",
    "# Concatenate DataFrames\n",
    "token_q = pd.concat([q_ling_features, tfidf_q], axis=1)\n",
    "token_q = token_q.drop(columns= ['blooms_verb_present'])\n",
    "\n",
    "print(classifier.predict(token_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'How many total disk access is needed to search a record using two level indexing?'\n",
    "inputs =  tokenizer([question], return_tensors='tf', padding=True, truncation=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract embeddings (CLS token for sentence representation)\n",
    "sentence_embedding = outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.733831</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.733293</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.733054</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 200, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.732995</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.732636</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 350, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.732337</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 350, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.731201</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.730544</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 200, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.730245</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 200, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.730006</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.730006</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 350, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.729468</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.729468</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 350, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.728990</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.728571</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 200, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.728512</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.727675</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.725941</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.725762</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.724985</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.723192</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 200, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.723132</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 350, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.722355</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.719546</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.718470</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.718171</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 350, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.717454</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.717095</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.716856</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 200, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.716557</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 350, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.715720</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.715421</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 200, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.715063</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.713927</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.710520</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.710042</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.708607</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 200, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.708249</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 350, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.707233</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.702630</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.701435</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.700658</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.699522</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.699103</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 350, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.697490</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 200, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.694082</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.689659</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.686372</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.684937</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.683981</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 350, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.683802</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 200, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.683503</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.681769</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.680634</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.679857</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.679259</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.678183</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 350, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.677884</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 200, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.677346</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.676987</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.673580</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.671488</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.669337</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.659295</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.656545</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.648834</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 200, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.647699</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.646145</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 350, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.646085</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.645487</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.639749</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.628870</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.555888</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.553437</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.551166</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.545666</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.539151</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.532636</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.529109</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.528452</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.526061</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.525822</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.523551</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.519187</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.424088</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.423849</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.422714</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.421578</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.420622</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.417932</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.417872</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.417633</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.416557</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.415959</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.415780</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.413150</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.336701</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.336701</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.336701</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.336581</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.336581</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.336402</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.331201</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.331142</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.331142</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.331082</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.330962</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.330663</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 100}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_test_score                                                                 params\n",
       "54          0.733831    {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2}\n",
       "48          0.733293        {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2}\n",
       "90          0.733054     {'criterion': 'entropy', 'max_depth': 200, 'min_samples_split': 2}\n",
       "102         0.732995     {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2}\n",
       "42          0.732636        {'criterion': 'gini', 'max_depth': 350, 'min_samples_split': 2}\n",
       "43          0.732337        {'criterion': 'gini', 'max_depth': 350, 'min_samples_split': 5}\n",
       "0           0.731201       {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2}\n",
       "36          0.730544        {'criterion': 'gini', 'max_depth': 200, 'min_samples_split': 2}\n",
       "91          0.730245     {'criterion': 'entropy', 'max_depth': 200, 'min_samples_split': 5}\n",
       "49          0.730006        {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5}\n",
       "96          0.730006     {'criterion': 'entropy', 'max_depth': 350, 'min_samples_split': 2}\n",
       "1           0.729468       {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5}\n",
       "97          0.729468     {'criterion': 'entropy', 'max_depth': 350, 'min_samples_split': 5}\n",
       "103         0.728990     {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5}\n",
       "37          0.728571        {'criterion': 'gini', 'max_depth': 200, 'min_samples_split': 5}\n",
       "55          0.728512    {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5}\n",
       "84          0.727675     {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2}\n",
       "30          0.725941        {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2}\n",
       "85          0.725762     {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5}\n",
       "2           0.724985      {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10}\n",
       "38          0.723192       {'criterion': 'gini', 'max_depth': 200, 'min_samples_split': 10}\n",
       "44          0.723132       {'criterion': 'gini', 'max_depth': 350, 'min_samples_split': 10}\n",
       "31          0.722355        {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5}\n",
       "50          0.719546       {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10}\n",
       "104         0.718470    {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10}\n",
       "98          0.718171    {'criterion': 'entropy', 'max_depth': 350, 'min_samples_split': 10}\n",
       "56          0.717454   {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10}\n",
       "3           0.717095      {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 20}\n",
       "92          0.716856    {'criterion': 'entropy', 'max_depth': 200, 'min_samples_split': 10}\n",
       "45          0.716557       {'criterion': 'gini', 'max_depth': 350, 'min_samples_split': 20}\n",
       "51          0.715720       {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 20}\n",
       "39          0.715421       {'criterion': 'gini', 'max_depth': 200, 'min_samples_split': 20}\n",
       "32          0.715063       {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10}\n",
       "86          0.713927    {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10}\n",
       "57          0.710520   {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 20}\n",
       "105         0.710042    {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 20}\n",
       "93          0.708607    {'criterion': 'entropy', 'max_depth': 200, 'min_samples_split': 20}\n",
       "99          0.708249    {'criterion': 'entropy', 'max_depth': 350, 'min_samples_split': 20}\n",
       "33          0.707233       {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 20}\n",
       "87          0.702630    {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 20}\n",
       "4           0.701435      {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 50}\n",
       "78          0.700658      {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2}\n",
       "52          0.699522       {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 50}\n",
       "46          0.699103       {'criterion': 'gini', 'max_depth': 350, 'min_samples_split': 50}\n",
       "40          0.697490       {'criterion': 'gini', 'max_depth': 200, 'min_samples_split': 50}\n",
       "79          0.694082      {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5}\n",
       "34          0.689659       {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 50}\n",
       "80          0.686372     {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10}\n",
       "58          0.684937   {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 50}\n",
       "100         0.683981    {'criterion': 'entropy', 'max_depth': 350, 'min_samples_split': 50}\n",
       "94          0.683802    {'criterion': 'entropy', 'max_depth': 200, 'min_samples_split': 50}\n",
       "24          0.683503         {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2}\n",
       "106         0.681769    {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 50}\n",
       "25          0.680634         {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5}\n",
       "81          0.679857     {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 20}\n",
       "88          0.679259    {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 50}\n",
       "47          0.678183      {'criterion': 'gini', 'max_depth': 350, 'min_samples_split': 100}\n",
       "41          0.677884      {'criterion': 'gini', 'max_depth': 200, 'min_samples_split': 100}\n",
       "53          0.677346      {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 100}\n",
       "5           0.676987     {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 100}\n",
       "26          0.673580        {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10}\n",
       "27          0.671488        {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 20}\n",
       "35          0.669337      {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 100}\n",
       "82          0.659295     {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 50}\n",
       "28          0.656545        {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 50}\n",
       "95          0.648834   {'criterion': 'entropy', 'max_depth': 200, 'min_samples_split': 100}\n",
       "59          0.647699  {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 100}\n",
       "101         0.646145   {'criterion': 'entropy', 'max_depth': 350, 'min_samples_split': 100}\n",
       "107         0.646085   {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 100}\n",
       "89          0.645487   {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 100}\n",
       "29          0.639749       {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 100}\n",
       "83          0.628870    {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 100}\n",
       "72          0.555888      {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 2}\n",
       "73          0.553437      {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 5}\n",
       "74          0.551166     {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 10}\n",
       "75          0.545666     {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 20}\n",
       "76          0.539151     {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 50}\n",
       "18          0.532636         {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 2}\n",
       "19          0.529109         {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 5}\n",
       "20          0.528452        {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10}\n",
       "21          0.526061        {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 20}\n",
       "77          0.525822    {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 100}\n",
       "22          0.523551        {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 50}\n",
       "23          0.519187       {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 100}\n",
       "66          0.424088      {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2}\n",
       "67          0.423849      {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5}\n",
       "68          0.422714     {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10}\n",
       "69          0.421578     {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 20}\n",
       "70          0.420622     {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 50}\n",
       "13          0.417932         {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5}\n",
       "12          0.417872         {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}\n",
       "14          0.417633        {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10}\n",
       "15          0.416557        {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 20}\n",
       "16          0.415959        {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 50}\n",
       "71          0.415780    {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 100}\n",
       "17          0.413150       {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 100}\n",
       "60          0.336701       {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2}\n",
       "62          0.336701      {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 10}\n",
       "61          0.336701       {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5}\n",
       "63          0.336581      {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 20}\n",
       "64          0.336581      {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 50}\n",
       "65          0.336402     {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 100}\n",
       "10          0.331201         {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 50}\n",
       "7           0.331142          {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 5}\n",
       "6           0.331142          {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 2}\n",
       "9           0.331082         {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 20}\n",
       "8           0.330962         {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 10}\n",
       "11          0.330663        {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 100}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'min_samples_split' : [2 , 5 , 10 , 20 , 50 , 100],\n",
    "    'criterion' : ['gini' , 'entropy'],\n",
    "    'max_depth' : [None , 5 , 10 , 20 , 50 , 100 , 200 , 350 , 500]\n",
    "    # 'min_samples_leaf' : [1 ,2 , 5 , 10 , 20 , 50 , 100]\n",
    "}\n",
    "param_search = GridSearchCV(estimator= DecisionTreeClassifier(), param_grid= params, cv=5)\n",
    "\n",
    "param_search.fit(x_resampled , y_resampled)\n",
    "\n",
    "cv_results = pd.DataFrame(param_search.cv_results_)\n",
    "cv_results = cv_results[[\"mean_test_score\", \"params\"]].head(10).sort_values(by=\"mean_test_score\", ascending=False)\n",
    "\n",
    "cv_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     analysis       0.80      0.83      0.82       564\n",
      "  application       0.82      0.82      0.82       564\n",
      "comprehension       0.78      0.79      0.78       554\n",
      "   evaluation       0.74      0.75      0.74       559\n",
      "    knowledge       0.71      0.70      0.70       547\n",
      "    synthesis       0.73      0.69      0.71       558\n",
      "\n",
      "     accuracy                           0.76      3346\n",
      "    macro avg       0.76      0.76      0.76      3346\n",
      " weighted avg       0.76      0.76      0.76      3346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(criterion= 'entropy', max_depth= None, min_samples_split = 2)\n",
    "dt.fit(x_train, y_train)\n",
    "y_pred = dt.predict(x_test)\n",
    "print(classification_report(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comprehension']\n"
     ]
    }
   ],
   "source": [
    "question = 'How many total disk access is needed to search a record using two level indexing?'\n",
    "processed_q = preprocess_text(question)\n",
    "tfidf_q = vectorizer.transform([processed_q])\n",
    "tfidf_q = pd.DataFrame(tfidf_q.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# Adding linguistic features\n",
    "q_ling_features = pd.DataFrame([extract_linguistic_features(question)])\n",
    "\n",
    "# Concatenate DataFrames\n",
    "token_q = pd.concat([q_ling_features, tfidf_q], axis=1)\n",
    "token_q = token_q.drop(columns= ['blooms_verb_present'])\n",
    "\n",
    "print(dt.predict(token_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mapper = {\n",
    "    'knowledge' : 0,\n",
    "    'comprehension' : 1,\n",
    "    'application' : 2,\n",
    "    'analysis' : 3,\n",
    "    'synthesis' : 4,\n",
    "    'evaluation' : 5\n",
    "}\n",
    "xgb_y = y_resampled.map(y_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m : [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti:softprob\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m : [\u001b[38;5;28;01mNone\u001b[39;00m , \u001b[38;5;241m5\u001b[39m , \u001b[38;5;241m10\u001b[39m , \u001b[38;5;241m50\u001b[39m , \u001b[38;5;241m100\u001b[39m , \u001b[38;5;241m200\u001b[39m , \u001b[38;5;241m500\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# 'subsample': [0.8, 0.9, 1.0]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m param_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39m XGBClassifier(), param_grid\u001b[38;5;241m=\u001b[39m params, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mparam_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_resampled\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxgb_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(param_search\u001b[38;5;241m.\u001b[39mcv_results_)\n\u001b[1;32m     13\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cv_results[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_test_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_test_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    967\u001b[0m         )\n\u001b[1;32m    968\u001b[0m     )\n\u001b[0;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/joblib/parallel.py:1985\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1984\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/joblib/parallel.py:1913\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1913\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:866\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 866\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/xgboost/sklearn.py:1682\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1660\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[1;32m   1661\u001b[0m     xgb_model, params, feature_weights\n\u001b[1;32m   1662\u001b[0m )\n\u001b[1;32m   1663\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1664\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1665\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1680\u001b[0m )\n\u001b[0;32m-> 1682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/xgboost/training.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_eve/lib/python3.10/site-packages/xgboost/core.py:2247\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2246\u001b[0m     _check_call(\n\u001b[0;32m-> 2247\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2250\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2252\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective' : ['multi:softprob'],\n",
    "    'max_depth' : [None , 5 , 10 , 50 , 100 , 200 , 500],\n",
    "    'n_estimators': [2 , 5 , 10 , 50 , 100, 200],\n",
    "    'learning_rate': [0.01, 0.1 , 0.5, 1]\n",
    "    # 'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "param_search = GridSearchCV(estimator= XGBClassifier(), param_grid= params, cv=5)\n",
    "\n",
    "param_search.fit(x_resampled , xgb_y)\n",
    "\n",
    "cv_results = pd.DataFrame(param_search.cv_results_)\n",
    "cv_results = cv_results[[\"mean_test_score\", \"params\"]].head(10).sort_values(by=\"mean_test_score\", ascending=False)\n",
    "\n",
    "cv_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_eve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
